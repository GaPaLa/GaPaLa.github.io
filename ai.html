<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv='cache-control' content='no-cache'> 
    <meta http-equiv='expires' content='0'> 
    <meta http-equiv='pragma' content='no-cache'>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GaPaLa Blog</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #fff;
        }

        header {
            text-align: center;
            padding: 20px;
            background-color: #bbb;
            color: #fff;
        }

        nav ul {
            list-style-type: none;
            padding: 0;
            margin: 0;
        }

        nav ul li {
            display: inline;
            margin-right: 20px;
        }

        nav ul li a {
            color: #fff;
            text-decoration: none;
            font-weight: bold;
        }

        nav ul li a:hover {
            color: #999;
        }

        .container {
            max-width: 1200px;
            margin: auto;
            padding: 20px;
        }
    </style>
</head>

<body>
    <header>
        <h1></h1>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="photography.html">Photography</a></li>
                <li><a href="misc.html">Misc.</a></li>
            </ul>
        </nav>
    </header>



    <div class="container">
        <h2 id="title">Here are the most important papers and thoughts on my mind atm</h2>
    
        <h3>Parallelism and Transformer Limitations</h3>
        <p>Architectures with parallelism across the sequence length may always force each target token to only have a fixed amount of compute available to be used for it, making solving multi-step problems past a certain algorithmic complexity impossible. Even with enough layers to fit the problem, it takes more layers than it would with recursion (hence why we see high redundancy across transformer layers - you need an addition function at layer N and N+1 to do 2 iterations of addition. I suspect), doesn't generalize to longer ones, and slows down learning ( I suspect. bc re-using the same layer recurrently gives it more informed gradients per param?). Suspicions are based on my own thoughts and <a href="https://arxiv.org/abs/2309.01826">One Wide Feedforward is All You Need</a></p>
        <ul>
            <li><a href="https://arxiv.org/abs/2404.08819">The Illusion of State in State-Space Models </a></li>
            <li><a href="http://www.wtf.sg/posts/2023-02-03-the-new-xor-problem/">The New XOR Problem</a></li>
        </ul>
    
        <p>This doesn't matter if you break down the problems in the dataset sufficiently such that the complexity required to calculate each token is within your limit, but we can't feasibly add CoT to a whole pretraining dataset.</p>
        <ul>
            <li><a href="https://arxiv.org/abs/2310.07923">The Expressive Power of Transformers with Chain of Thought</a></li>
            <li><a href="https://arxiv.org/abs/2307.03381">Teaching Arithmetic to Small Transformers</a></li>
        </ul>

        <p>You can't just add blank tokens though - the CoT tokens have to meaningfully bring information back down, acting as recurrence in place of having model do this internally.</p>
        <ul>
            <li><a href="https://arxiv.org/abs/2404.15758v1">Let's Think Dot by Dot: Hidden Computation in Transformer Language Models</a> (the related work section clears this up well)
        </ul>
    
        <h3>Limits of SGD's Ability to Generalize OOD</h3>
        <ul>
            <li><a href="https://arxiv.org/pdf/2210.05075">The "A is B Reversal Curse"</a></li>
            <li>(+ my own work (coming soon!) that confirms the above and tries to make use of ICL to work side-by-side with SGD across pretraining).</li>
        </ul>
        
        <p>More work around this topic is done under the title of "Memory-Augmented Neural Networks" - MANNs.</p>
        <ul>
            <li><a href="https://proceedings.mlr.press/v48/santoro16.pdf">Meta-Learning with Memory-Augmented Neural Networks</a></li>
        </ul>
    
        <h3>Is Polysemanticity the cause of adversarial brittleness?</h3>
        <p>I think this is different from OOD issues because this enables adversarial examples with very minimal changes, which I would count as iid. I think it's that polysemanticity --> features aren't orthogonal --> there is always a non-zero gradient to get any behaviour from some input perturbation that is irrelevant to that behaviour.</p>
        <ul>
            <li><a href="https://arxiv.org/abs/2209.10652">Toy Models of Superposition</a></li>
        </ul>
    

        <h3>Also, we need alternatives to next-token prediction</h3>
        <p>LM loss encourages sticking IID, but we actually want the model to continually achieve new, harder-to-reach goals that it has never seen before, which will require doing new things it hasn't seen before. Also avoids degeneracy of teacher-forcing.</p>
        <ul>
            <li><a href="https://arxiv.org/abs/2311.14648">Calibrated Language Models Must Hallucinate</a></li>
            <li><a href="https://www.youtube.com/watch?v=hhiLw5Q_UFg">Talk on RLHF vs finetuning by John Schulman</a></li>
        </ul>
    
    </div>
    

</html>










</body>

</html>


