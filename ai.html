<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv='cache-control' content='no-cache'> 
    <meta http-equiv='expires' content='0'> 
    <meta http-equiv='pragma' content='no-cache'>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GaPaLa Blog</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #fff;
        }

        header {
            text-align: center;
            padding: 20px;
            background-color: #bbb;
            color: #fff;
        }

        nav ul {
            list-style-type: none;
            padding: 0;
            margin: 0;
        }

        nav ul li {
            display: inline;
            margin-right: 20px;
        }

        nav ul li a {
            color: #fff;
            text-decoration: none;
            font-weight: bold;
        }

        nav ul li a:hover {
            color: #999;
        }

        .container {
            max-width: 1200px;
            margin: auto;
            padding: 20px;
        }
    </style>
</head>

<body>
    <header>
        <h1>GaPaLa Blog</h1>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="photography.html">Photography</a></li>
                <li><a href="misc.html">Misc.</a></li>
            </ul>
        </nav>
    </header>



    <div class="container">
        <h2 id="title">Key thoughts</h2>
        <h4>(warning: lots of jargon ahead. Requires reading on deep learning in general; the transformer architecture; causal language models; chain-of-thought prompting; intuition on Chollet's 'On The Measure Of Intelligence' (see his latest appearance on Dwarkesh Patel's podcast); shallow but broad intuition of deep reinforcement learning)</h4>
    
        <h3>Limitations to the Expressiveness of Parallel Architectures (e.g. Transformers/SSMs) <bold>- 2025 update: NVM </bold> </h3>
        <p>Architectures which are parallelisable across the length of the input sequence may fix the amount of compute available to calculate each given target token, making solving multi-step problems past a certain algorithmic space/time complexity (for a sufficiently large N in O(N)) impossible. Even with enough layers to fit the problem, it takes more layers (i.e. parameters - not including recursion) than it would with recursion (hence why we see high redundancy across transformer layers - given that a single layer cannot fit more than one addition circuit, you would need an addition function at layer N and N+k to do 2 iterations of addition, as opposed to re-using a single implementation of addition repeatedly.); it doesn't generalize to longer sequences than those trained on; and I suspect it slows down learning (re-using the same layer recurrently gives it more informed gradients per param and per daata sample?). Suspicions are based on my own thoughts and <a href="https://arxiv.org/abs/2309.01826">One Wide Feedforward is All You Need</a></p>
        <ul>
            <li><a href="https://arxiv.org/abs/2404.08819">The Illusion of State in State-Space Models </a></li>
            <li><a href="http://www.wtf.sg/posts/2023-02-03-the-new-xor-problem/">The New XOR Problem</a></li>
        </ul>
    
        <p>This depth-ceiling expressivity issue doesn't matter if you break down the problems in the dataset sufficiently across multiple tokens such that the complexity required to calculate each individual token is within the depth limit, but we can't feasibly add CoT tokens with these space/time complexity guarantees to a whole pretraining dataset. Even if we could, transformers don't seem to generalise comparably to humans from the data they learn - will CoT data be any different? From what I know of CoT papers, it doesn't eem to be a skill for which they do generalise better than any other.</p>
        <ul>
            <li><a href="https://arxiv.org/abs/2310.07923">The Expressive Power of Transformers with Chain of Thought</a></li>
            <li><a href="https://arxiv.org/abs/2307.03381">Teaching Arithmetic to Small Transformers</a></li>
        </ul>


        <p>You also can't just add blank tokens - the CoT tokens have to meaningfully bring information 'back down' from target to input, acting as recurrence in place of having the model do this internally.</p>
        <ul>
            <li><a href="https://arxiv.org/abs/2404.15758v1">Let's Think Dot by Dot: Hidden Computation in Transformer Language Models</a> (the related work section clears this up well)
        </ul>
		
		<p> (As of 2025) My worries about the difficulty of acquiring large datasets of meaningful CoT tokens and generalising from them seem to have been empirically disproven to some extent by the performance OpenAI's O3 model. I was doubtful that synthetic CoT data generated by current LLMs would be able to provide CoT tokens meaningful enough to break down the algorithmic complexity of many problems across the context length. I was also doubtful that LLMs could even learn from such data how to acquire that general skill of breaking problems down to reduce their complexity for themselves in order to solve them. However, OpenAI's use of reinforcement-learned CoT abilities and the resulting "O3" model has made such unpredictably great progress on Francois Chollet's Abstract Reasoning Corpus (the only benchmark I am aware of that really tests for robust, adaptive intelligence rather than raw skill) and a private math dataset so difficult and OOD that I dismissed its usefulness (EpochAI's Frontier Math), that I can no longer say that this method likely faces these limitations. There are still caveats - the amount of CoT tokens to solve ARC is so high that just the inference costs $20k across millions of tokens. Importantly, it seems they ran the model many, many times (1024) for each problem (using the "high compute" mode which reached human performance). It therefore seems more of a brute force solution, and whether that brute force search was done by the model itself or their use of the model makes a big difference. If by the model, training at a larger scale will likely teach it more effective strategies for search (think binary trees - asking critical questions which reduce the possible space of solutions). If not, then the method may be dead in the water as it will likely not get 1000x cheaper (which is needed to get close to how cheap human labour is imo) and likely doesn't offer many advantages compared to querying a standard model many times. Additionally, it seems OpenAI has access to the MathFrontier dataset - there are questions of if they trained on it, which would mean its Math Frontier score no longer indicates that it can indeed go far OOD. But we don't know the answer to that. And until the full O3 model comes out and we can see its CoT more or less, it will be hard to determine how intelligently it has learned to do this search process. So really, I think we're still in the dark with how far these models can go. There's a nice intuitive visualisation of the concept of generalisation in Chollet's "On the Measure of Reasoning". In it, we see several circles representing the area that our training datasets cover of all problems in the world. Extending from these are larger encompassing circles, whose shaded area indicates the problems they can solve. A system which does not generalise will learn to cover only the area covered by the data, and not extend out from its spheres at all. A system which generalises far will shade a much larger region, extending far out from the circles covered by its training data. In the evolutionary sense, humans' training data covered only hunter-gatherer lifestyles and problems, and yet we as a species have reached the moon and designed computers and art with no end to sophistication or breadth. Transformers are trained not only on data of hunter-gatherer lifestyles, but our entire rich history and our modern mathematics and science and arts, and yet it fails to do 3-digit multiplication or come up with any moving poem by my fairly generous standards. I will say that diffusion image generators are impressive. Point being: transformers have shown to reach a only very small shaded area outside of their training data, not even reaching its circumference in most places (afaik). However, I think 'reasoning' models likely branch further out. How much further? I still seriously doubt it is anywhere human. But of course, what does it matter if they don't reach human-level generalisation if they have enough data such that the shaded region they create really does cover most of the problems we want to solve, 99.99% of the time? Whether they can reach this threshold is an empirical question that I think will only be solved by the 100x scale GPU clusters of the future which eat powerplants. What's even more interesting, is that if they do, we may reach high levels of automation without AGI ever being needed (to the great quelling of most worries about AI alignment).

        <h3>Limits of SGD's Ability to Generalize OOD</h3>
        <ul>
            <li><a href="https://arxiv.org/pdf/2210.05075">The "A is B Reversal Curse"</a></li>
            <li>(+ my undergrad diss that confirms the above and tries (& fails) to make use of ICL to work side-by-side with SGD across pretraining).</li>
        </ul>
        
        <p>More work around this topic is done under the title of "Memory-Augmented Neural Networks" - MANNs.</p>
        <ul>
            <li><a href="https://proceedings.mlr.press/v48/santoro16.pdf">Meta-Learning with Memory-Augmented Neural Networks</a></li>
        </ul>
    
        <h3>Is Polysemanticity the cause of adversarial brittleness?</h3>
        <p>I think this is different from OOD issues because this enables adversarial examples with very minimal changes, which I would count as iid. I think it's that polysemanticity --> features aren't orthogonal --> there is always a non-zero gradient to get any behaviour from some input perturbation that is irrelevant to that behaviour.</p>
        <ul>
            <li><a href="https://arxiv.org/abs/2209.10652">Toy Models of Superposition</a></li>
        </ul>
    

        <h3>Also, we need alternatives to next-token prediction - (2025 update: things seem to be going in this direction now with the use of RL)</h3>
        <p>LM loss encourages sticking IID, but we actually want the model to continually achieve new, harder-to-reach goals that it has never seen before, which will require doing new things it hasn't seen before. Also avoids the inconsistent internal hiddens states which teacher-forcing encourages (if you read half a sentence and think the rest of it may go one way but it actually goes another, the transformer is forced to learn how to consider both options to always get both right and so its internal state is forced to not commit - humans have it much easier). Also, if you are trained on next-token prediciton, making educated guesses and confidently declaring them as fact gets better performance than saying "I don't know". </p>
        <ul>
            <li><a href="https://arxiv.org/abs/2311.14648">Calibrated Language Models Must Hallucinate</a></li>
            <li><a href="https://www.youtube.com/watch?v=hhiLw5Q_UFg">Talk on RLHF vs finetuning by John Schulman</a></li>
        </ul>
        <p>Active inference seems to be the way forward. Just need an architecture with expressive and efficient state-tracking; long-range meta-learning (continual learning); a good RL environment; and an efficient training pipeline. So, everything :/\/\/</p>
    
    </div>
    

</html>










</body>

</html>


