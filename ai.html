<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv='cache-control' content='no-cache'> 
    <meta http-equiv='expires' content='0'> 
    <meta http-equiv='pragma' content='no-cache'>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GaPaLa Blog</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #fff;
        }

        header {
            text-align: center;
            padding: 20px;
            background-color: #bbb;
            color: #fff;
        }

        nav ul {
            list-style-type: none;
            padding: 0;
            margin: 0;
        }

        nav ul li {
            display: inline;
            margin-right: 20px;
        }

        nav ul li a {
            color: #fff;
            text-decoration: none;
            font-weight: bold;
        }

        nav ul li a:hover {
            color: #999;
        }

        .container {
            max-width: 1200px;
            margin: auto;
            padding: 20px;
        }
    </style>
</head>

<body>
    <header>
        <h1></h1>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="photography.html">Photography</a></li>
                <li><a href="misc.html">Misc.</a></li>
            </ul>
        </nav>
    </header>



    <div class="container">
        <h2 id="title">Here are the most important papers and thoughts on my mind atm</h2>
    
        <h3>Parallelism and Transformer Limitations</h3>
        <p>Architectures with parallelism across the sequence length may force each target token to only have a fixed amount of compute available to compute it, making solving multi-step problems (generalizably) past a certain algorithmic complexity impossible, or take more layers than it would with recursion (slowing down learning?).</p>
        <ul>
            <li>The Illusion of State in State-Space Models - <a href="https://arxiv.org/abs/2404.08819">https://arxiv.org/abs/2404.08819</a></li>
            <li>The New XOR Problem - <a href="https://openreview.net/forum?id=HgOJlxzB16">https://openreview.net/forum?id=HgOJlxzB16</a></li>
        </ul>
    
        <p>This doesn't matter if you break down the problems in the dataset sufficiently such that the complexity required to calculate each token is within your limit, but obvs we can't just write a CoT pretraining dataset.</p>
        <ul>
            <li>The Expressive Power of Transformers with Chain of Thought - <a href="https://iopscience.iop.org/article/10.1088/1361-648X/ab8849/meta">https://iopscience.iop.org/article/10.1088/1361-648X/ab8849/meta</a></li>
            <li>Teaching Arithmetic to Small Transformers - <a href="https://arxiv.org/abs/2307.03381">https://arxiv.org/abs/2307.03381</a></li>
            <li>You can't just add blank tokens though, they have to meaningfully bring information back down, acting as recurrence in place of having model do this internally <a href="https://arxiv.org/abs/2404.08819">https://arxiv.org/abs/2404.08819</a>, <a href="https://www.researchgate.net/publication/373318425_Structured_Kernel_Estimation_for_Photon-Limited_Deconvolution">https://www.researchgate.net/publication/373318425_Structured_Kernel_Estimation_for_Photon-Limited_Deconvolution</a></li>
        </ul>
    
        <h3>Limits of SGD's Ability to Generalize OOD</h3>
        <ul>
            <li>The "'A is B' Reversal Curse" - <a href="https://arxiv.org/pdf/2210.05075">https://arxiv.org/pdf/2210.05075</a></li>
            <li>(+ my own work coming out soon that confirms the above and tries to make use of ICL to work side-by-side with SGD across pretraining)</li>
        </ul>
        
        <p>More work around this topic is done under the title of "Memory-Augmented Neural Networks" - MANNs</p>
        <ul>
            <li>Meta-Learning with Memory-Augmented Neural Networks - <a href="https://proceedings.mlr.press/v48/santoro16.pdf">https://proceedings.mlr.press/v48/santoro16.pdf</a></li>
        </ul>
    
        <h3>Is Polysemanticity the cause of adversarial brittleness?</h3>
        <ul>
            <li>Toy Models of Superposition: This paper explores toy models related to superposition - <a href="https://arxiv.org/abs/2209.10652">https://arxiv.org/abs/2209.10652</a></li>
        </ul>
    

        <h3> Also, we need alternatives to next-token prediction</h3>
        <p> LM loss encourages sticking IID, but we actually want the model to continually achieve new, harder-to-reach goals that it has never seen before, which will require doing new things it hasn't seen before. Also avoids degeneracy of teacher-forcing.</p>
        <ul>
            <li>Calibrated Language Models Must Hallucinate - <a href="https://arxiv.org/abs/2311.14648">https://arxiv.org/abs/2311.14648</a></li>
            <li>RL > LM - <a href="https://www.youtube.com/watch?v=hhiLw5Q_UFg">https://www.youtube.com/watch?v=hhiLw5Q_UFg</a></li>
        </ul>
    
    </div>
    

</html>










</body>

</html>


