<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv='cache-control' content='no-cache'> 
    <meta http-equiv='expires' content='0'> 
    <meta http-equiv='pragma' content='no-cache'>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GaPaLa Blog</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #fff;
        }

        header {
            text-align: center;
            padding: 20px;
            background-color: #bbb;
            color: #fff;
        }

        nav ul {
            list-style-type: none;
            padding: 0;
            margin: 0;
        }

        nav ul li {
            display: inline;
            margin-right: 20px;
        }

        nav ul li a {
            color: #fff;
            text-decoration: none;
            font-weight: bold;
        }

        nav ul li a:hover {
            color: #999;
        }

        .container {
            max-width: 1200px;
            margin: auto;
            padding: 20px;
        }
    </style>
</head>

<body>
    <header>
        <h1>GaPaLa Blog</h1>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="photography.html">Photography</a></li>
                <li><a href="misc.html">Misc.</a></li>
            </ul>
        </nav>
    </header>



    <div class="container">
        <h2 id="title">Here are the most important papers and thoughts on my mind atm</h2>
        <h4>(warning: lots of jargon ahead. Requires reading on deep learning in general; the transformer architecture; causal language models; chain-of-thought prompting; intuition on Chollet's 'On The Measure Of Intelligence' (see his latest appearance on Dwarkesh Patel's podcast); shallow but broad intuition of deep reinforcement learning)</h4>
    
        <h3>Limitations to the Expressiveness of Parallel Architectures (e.g. Transformers/SSMs)</h3>
        <p>Architectures which are parallelisable across the length of the input sequence may fix the amount of compute available to calculate each given target token, making solving multi-step problems past a certain algorithmic space/time complexity (for a sufficiently large N in O(N)) impossible. Even with enough layers to fit the problem, it takes more layers (i.e. parameters - not including recursion) than it would with recursion (hence why we see high redundancy across transformer layers - given that a single layer cannot fit more than one addition circuit, you would need an addition function at layer N and N+k to do 2 iterations of addition, as opposed to re-using a single implementation of addition repeatedly.); it doesn't generalize to longer sequences than those trained on; and I suspect it slows down learning (re-using the same layer recurrently gives it more informed gradients per param and per daata sample?). Suspicions are based on my own thoughts and <a href="https://arxiv.org/abs/2309.01826">One Wide Feedforward is All You Need</a></p>
        <ul>
            <li><a href="https://arxiv.org/abs/2404.08819">The Illusion of State in State-Space Models </a></li>
            <li><a href="http://www.wtf.sg/posts/2023-02-03-the-new-xor-problem/">The New XOR Problem</a></li>
        </ul>
    
        <p>This depth-ceiling expressivity issue doesn't matter if you break down the problems in the dataset sufficiently across multiple tokens such that the complexity required to calculate each individual token is within the depth limit, but we can't feasibly add CoT tokens with these space/time complexity guarantees to a whole pretraining dataset.</p>
        <ul>
            <li><a href="https://arxiv.org/abs/2310.07923">The Expressive Power of Transformers with Chain of Thought</a></li>
            <li><a href="https://arxiv.org/abs/2307.03381">Teaching Arithmetic to Small Transformers</a></li>
        </ul>

        <p>You also can't just add blank tokens - the CoT tokens have to meaningfully bring information 'back down' from target to input, acting as recurrence in place of having the model do this internally.</p>
        <ul>
            <li><a href="https://arxiv.org/abs/2404.15758v1">Let's Think Dot by Dot: Hidden Computation in Transformer Language Models</a> (the related work section clears this up well)
        </ul>
    
        <h3>Limits of SGD's Ability to Generalize OOD</h3>
        <ul>
            <li><a href="https://arxiv.org/pdf/2210.05075">The "A is B Reversal Curse"</a></li>
            <li>(+ my own work (coming soon!) that confirms the above and tries to make use of ICL to work side-by-side with SGD across pretraining).</li>
        </ul>
        
        <p>More work around this topic is done under the title of "Memory-Augmented Neural Networks" - MANNs.</p>
        <ul>
            <li><a href="https://proceedings.mlr.press/v48/santoro16.pdf">Meta-Learning with Memory-Augmented Neural Networks</a></li>
        </ul>
    
        <h3>Is Polysemanticity the cause of adversarial brittleness?</h3>
        <p>I think this is different from OOD issues because this enables adversarial examples with very minimal changes, which I would count as iid. I think it's that polysemanticity --> features aren't orthogonal --> there is always a non-zero gradient to get any behaviour from some input perturbation that is irrelevant to that behaviour.</p>
        <ul>
            <li><a href="https://arxiv.org/abs/2209.10652">Toy Models of Superposition</a></li>
        </ul>
    

        <h3>Also, we need alternatives to next-token prediction</h3>
        <p>LM loss encourages sticking IID, but we actually want the model to continually achieve new, harder-to-reach goals that it has never seen before, which will require doing new things it hasn't seen before. Also avoids degeneracy of teacher-forcing.</p>
        <ul>
            <li><a href="https://arxiv.org/abs/2311.14648">Calibrated Language Models Must Hallucinate</a></li>
            <li><a href="https://www.youtube.com/watch?v=hhiLw5Q_UFg">Talk on RLHF vs finetuning by John Schulman</a></li>
        </ul>
        <p>Active inference seems to be the way forward. Just need an architecture with expressive and efficient state-tracking; long-range meta-learning (continual learning); a good RL environment; and an efficient training pipeline. So, everything :/\/\/</p>
    
    </div>
    

</html>










</body>

</html>


